  <div class="row">
    <h2>Experience</h2>


    <hr>
    <div class="row">
      <div class="col-sm-12">
        <h3 class="content_heading">Visual document parsing for engineering blueprints</em></h3>
        <br>
        <b>Context/Challenge:</b> Information from Engineering Drawings (like P&IDs) is challenging to extract because: <br>
          <ul>
            <li> They are often shared in image-based formats (e.g., PDF, PNG, JPEG) rendering CAD software tools unable to index the information.</li> 
            <li> A lot of information in Engineering Drawings is 'visual' in nature which requires domain expertise and topological understanding to understand how different objects are functionally connected to one another.</li>
          </ul>
          <b>Solution:</b> Made the <b>'visual' information within P&IDs searchable</b> using LLMs and computer vision. See Figure below. <br>
        <!-- Consequently, P&IDs are first digitized by identifying entities such as symbols, lines, and text (Step-<b>I</b>). The detected entities are then linked to create a base entity graph (Step-<b>II</b>). Here, symbols = nodes, lines = edges, texts = property. The base entity graph is then augmented with semantic attributes for nodes and edges forming knowledge graph (Step-<b>III</b>). The knowledge graph is then used to answer queries such as "find all the valves in the drawing" or perform design checks like "are pumps provided with strainers to prevent dirt getting in? (Step-<b>IV</b>)".<br> -->
        <br>
        <figure>
        <img src="static/img/pid_parsing.png" width="850px">
        <figcaption>Method overview. Step-I: creation of a base entity graph, Step-II: conversion into a labeled property graph, and Step-III: RAG-based information retrieval system.</figcaption>
        </figure>

        <b>Contributions:</b><br>
        (1) A semi-supervised framework to detect symbols that reduced resource requirements to label dataset by 65%. Used Class-Agnostic Detection followed by one-shot classification. <b>Intuition:</b> No one wants to label more than 400 different types of symbols and symbols look the same across drawings (except orientation & scale) + previous research suggests Class-agnostic training results in a 'generalizable' detector able to localize unseen symbol categories. For more details and results: <strong><mark>Published Papers</mark></strong> -
        <a target="_blank" href="https://www.sciencedirect.com/science/article/abs/pii/S0926580523005204">[Autom. Constr. 2024]</a>,
        <a target="_blank" href="https://www.iaarc.org/publications/2022_proceedings_of_the_39th_isarc_bogota_colombia/automated_valve_detection_in_piping_and_instrumentation_pid_diagrams.html"> [ISARC 2022]</a> <br>
        (2) A custom line detection method <em>(Most tricky)</em> using Probabilistic Hough Transform with my own line-merging and programmatic hyperparameter selection modules. It finds all lines in a 7168x4561 image within 22 seconds with no manual intervention (compared to few minutes in common pixel-by-pixel traversal methods). [Paper] (Accepted) posting soon....<br>
        (3) A thorough eveluation of LLMs in answering four types of questions - simple counting, spatial counting, connectivity-related and value-based. Methods to improve Text-to-Cypher accuracy such as graph schema reshaping, contextualization, few-shot prompting are quanitified. [Paper] (Accepted) posting soon....<br>
        
      </div>
    </div>

    <hr>
    <div class="row">
     <div class="col-sm-12">
       <h3 class="content_heading">Condition Assessment - Monitoring Vibrations in a Hydro-Electric Dam </h3>
       <img src="https://img.shields.io/badge/Digital%20Twins-cyan" alt="DT Badge">
       <img src="https://img.shields.io/badge/Predictive%20Maintenance-grey" alt="PdM Badge">
       <img src="https://img.shields.io/badge/Anomaly%20Detection-green" alt="AD Badge">
       <img src="https://img.shields.io/badge/Recurrent%20Neural%20Network-orange" alt="RNN Badge">
       <img src="https://img.shields.io/badge/Performance%20Curve%20Modeling-blue" alt="PCM Badge">
       <br>
       <br>
       <a class="github-button btn" href="https://github.com/mgupta70/Predictive-Maintenance-Data-Analytics-App" data-size="large" data-show-count="false"
        aria-label="mgupta70/Predictive-Maintenance-Data-Analytics-App on GitHub">Code</a>
      <a class="github-button" href="https://github.com/mgupta70/Predictive-Maintenance-Data-Analytics-App" data-icon="octicon-star" data-size="large"
        data-show-count="true" aria-label="Star mgupta70/Predictive-Maintenance-Data-Analytics-App on GitHub">Star</a>
        <br>
        <figure>
        <img src="static/img/app_overview_svg.svg" width="750px">
      </figure>
      <br>
       <p><strong><mark>Link to Webapp</mark></strong> -
         <a target="_blank" href="https://predictive-maintenance-time-series.streamlit.app/">[Condition Monitoring App]</a>,
       </p>
       <br>
       <p><b>Context:</b> The project is part of a larger initiative to develop a digital twin of the dam. The digital twin will be used to monitor the health of the dam and its components, enabling predictive maintenance and reducing downtime.<br> 
        <b>Challenge:</b> The biggest challenge was the absence of 'faulty' vibration data as most of the time, turbine was operated in normal (safe) working condition.
        <b>Solution:</b> To solve this, I engaged with field engineers to understand the data and integrate domain knowledge to buid a model for estimating <del> vibration at any time t in future</del> <b>rate of change of vibration</b>. Developed a web application that allows users to visualize the data and perform anomaly detection and time-series forecasting. The app uses a custom ANN-LSTM to predict the future values of the vibration data, and a performance curve model to detect anomalies. The app also includes a dashboard that displays the current status of the dam and its components, as well as alerts for any anomalies detected. </p>
       
     </div>
   </div>


    <hr>
    <div class="row">
      <div class="col-sm-12">
        <h3 class="content_heading">3D scene reconstruction with NeRF and SfM from 360Â°camera images</h3>
        <img src="https://img.shields.io/badge/NerfStudio-yellow" alt="Nerf Badge">
        <img src="https://img.shields.io/badge/Colmap-orange" alt="Colmap Badge">
        <img src="https://img.shields.io/badge/Cloud%20Compare-blue" alt="CloudCompare Badge">
        <img src="https://img.shields.io/badge/Keypoint%20Matching-green" alt="Keypoint Badge">
        <br>
        <br>
        <p><strong><mark>Published Paper</mark></strong> -
          <a target="_blank" href="https://drive.google.com/file/d/1Mc4nvLkxQQfcEw4aE41MP5FPK6P2ftfT/view?usp=drive_link">[Paper]</a>
        </p>
        <p> Capturing a scene with Lidar inherently needs a large amount of data storage. So, in this study, I investigated an alternative approach to capture the 3D information via 360 videos. I particularly used Structure-from-Motion (SfM) and Neural Radiance Fields (NeRF) for the task of 3D reconstruction. These techniques provide significant savings in terms of data storage requirements but there is a trade-off with dimensional accuracy. Please refer the Results section in the attached paper for quantitative results. 
          Since construction sites are not always well-lit, NeRF didn't perform quite well for the objects that were far and poorly lit. However, areas which are well-lit, 3D reconstruction by NeRF and SfM were almost same.
          In my view, the selection of appropriate technology would be driven by its use case. For ex: for a simple task of visualization, SfM method could be preferred but if one needs precise measurement (of tiny or thin objects) then Lidar would be the choice.</p>
      </div>
    </div>
    <div class="row" align="center">
  <div class="col-sm-4">
    <figure>
      <img src="static/img/small_scan.gif" width="300px">
      <figcaption>Laser scan</figcaption>
    </figure>
  </div>
  <div class="col-sm-4">
    <figure>
      <img src="static/img/small_nerf.gif" width="300px">
      <figcaption>NeRF</figcaption>
    </figure>
  </div>
  <div class="col-sm-4">
    <figure>
      <img src="static/img/small_colmap.gif" width="300px">
      <figcaption>SfM</figcaption>
    </figure>
  </div>
</div>

<hr>
    <div class="row">
      <div class="col-sm-12">
        <h3 class="content_heading">Sleep Posture Correction to Alleviate Snoring</h3>
        <img src="https://img.shields.io/badge/Spectrogram-grey" alt="Spectrogram Badge">
        <img src="https://img.shields.io/badge/Arduino%20Nano%20BLE%20Sense-blue" alt="BLE Badge">
        <img src="https://img.shields.io/badge/Embedded%20Machine%20Learning-orange" alt="EML Badge">
        <img src="https://img.shields.io/badge/Relay-green" alt="Relay Badge">
        <img src="https://img.shields.io/badge/Mini%20Compressor-indigo" alt="MC Badge">
        <br>
        <br>
      <a class="github-button btn" href="https://github.com/mgupta70/Embedded-ML-Snoring-Relief-Pillow" data-size="large" data-show-count="false"
        aria-label="mgupta70/Embedded-ML-Snoring-Relief-Pillow on GitHub">Code</a>
        <br>
        <br>
        <p> Integrated smart sensing into a Pillow by connecting it with an air compressor to adjust person's  position, once snoring is detected. 
          Snoring detection is performed by image classification of audio signals converted into spectrogram. Model is light-weight and runs on Arduino Nano 33 BLE, embedded in the pillow. 
        </p>
      </div>
    </div>

    <div style="display: flex; justify-content: space-between;">
      <figure>
        <img src="static/img/Components.png" alt="Components" width="300">
        <figcaption>Components</figcaption>
      </figure>
    
      <figure>
        <img src="static/img/circuit_design.png" alt="Circuit Design" width="300">
        <figcaption>Circuit Design</figcaption>
      </figure>
    
      <figure>
        <img src="static/img/Setup.png" alt="Setup" width="300">
        <figcaption>Setup</figcaption>
      </figure>
    </div>
</div>

    
    <hr>
     <div class="row">
      <div class="col-sm-12">
        <h3 class="content_heading">Automated 3D digital models from 2D CAD plans</h3>
        <img src="https://img.shields.io/badge/Faster%20RCNN-pink" alt="RCNN Badge">
        <img src="https://img.shields.io/badge/Yolov4-orange" alt="Yolov4 Badge">
        <img src="https://img.shields.io/badge/Dynamo%20API-green" alt="Dynamo Badge">
        <img src="https://img.shields.io/badge/Autodesk-blue" alt="Autodesk Badge">
        <br>
        <br>
        <p><strong><mark>Published Papers</mark></strong> -
          <a target="_blank" href="https://www.mdpi.com/2075-5309/13/9/2336">[Paper-1]</a>,
          <a target="_blank" href="https://www.iaarc.org/publications/2022_proceedings_of_the_39th_isarc_bogota_colombia/automated_wall_detection_in_2d_cad_drawings_to_create_digital_3d_models.html"> [Paper-2]</a>
        </p>
        <br>
        <p>  Aim of this project is to speed up the conversion of a 2D CAD into a 3D BIM model. We found that the semantic segmentation-based approach is more performant than the typical bounding-based methods. In post-processing, we configured the linkage of neural network outputs to a 3D drafting software via Dynamo API.</p>
        <!-- <figure>
          <img src="static/img/cad_to_3d.png" alt="wall3d" width="800">
        </figure> -->
        <div style="display: flex; justify-content: center; gap: 20px;">
          <figure>
            <img src="static/img/cad_overview.png" width="700px">
          </figure>
          <figure>
            <img src="static/img/cad_to_3d.png" width="300px">
          </figure>
        </div>
        
      </div>
    </div>


</div>
     
